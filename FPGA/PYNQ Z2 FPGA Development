Xilinx Pynq FPGA Development Cheatsheet
//pynq.readthedocs.io/en/v2.5/pynq_package/pynq.lib/pynq.lib.pmod.html
//https://github.com/Xilinx/PYNQ_Workshop
//https://groups.google.com/forum/#!forum/pynq_project
//CNN on PYNQ - https://www.youtube.com/watch?v=DoA8hKBltV4
//https://pynq.readthedocs.io/en/v2.0/overlay_design_methodology/python_packaging.html
//https://github.com/hillhao/PYNQ-project#5-neural-network-design
//////////////////////////////////////////////////////////////////////////////////////////////////
//    ∑ 𝒳𝑥 𝒲𝑤 𝑏 𝑓ƒ 𝒰𝑢
//////////////////////////////////////////////////////////////////////////////////////////////////
CONTENTS
	01. Introduction
			1.1 The Board
			1.2 Zynq7000
			1.3 Starting Up
			1.4 Revert Board to Factory settings
			1.5 Access the terminal of PYNQ
			1.6 Jupyter Notebook
			1.7 Grove Temperature Sensor v1.2 Example
	02. PYNQ with Python OpenCV for Image/Video Processing
			2.1 Programming, Conditional Statements, Loops with GPIO
			2.2 OpenCV Development - OpenCV Basics
			2.3 Face Eye Detection
			2.4 HDMI Streaming and Processing
			2.5 License Plate Localizer
	03. Installing Pyhton Library in PYNQ - Cryptography Library
	04. Machine Learning with Python in PYNQ
			4.1 Character Recognition with BNN
	05. Creating Custom Overlay (VIVADO Project) for PYNQ
			5.1 Custom Overlay: Addition and Multiplication App
			5.2 How to create a Custom Overlay
			5.3 Creating VDMA Overlay with VIVADO and Notebook
	06. Creating Custom Python Function Accelerator on PYNQ with VIVADO tool
			6.1 Accelerate Custom Image Processing Function
	07. Tensorflow Installation on PYNQ FPGA 
	08. Machine Learning with Xilinx Deep Learning (DPU) IP on PYNQ 
			8.1 DPU on PYNQ Boards: Ultra96/ZCU104/ZCU111
	09. Blinking LED with Vivado + Vitis
	
//////////////////////////////////////////////////////////////////////////////////////////////////
01. Introduction
	
	1.1 The Board
		+----+-+----+--+----+------------------------------------------------------+
		| 01 | | 02 |  | 03 |           +-------------------------------+    +-----|
		|----+ +----+  +----+           |              04               |    | 05  |
		|                               +-------------------------------+    +-----|
		|                          +---------------------------------------+       |   +---------------+       
		|           Arm            | +-----------------------------------+ | +-----|   | A | B | C | D |
		|           +----------+   | |                                   | | | 06  |   |---------------|
		|           |          |   | +-----------------------------------+ | +-----|   | E | F | G | H |    
		|----+      | Zynq7000 |   |                                 +---+ |       |   +---------------+ 
		| 17 |      |          |   |                                 |   | | +-----|   A+B: Load from SD
		|----+      +----------+   |                 [09]            |   | | | 07  |
		|           PL             |                                 +---+ | +-----|
		|----+                     |         +---------------------------+ |       |
		| 16 |                     |         |                           | | +-----|
		|----+                     |         +---------------------------+ | | 08  |
		|          [26]     [27]   +---------------------------------------+ +-----|    
		|----+    +----+   +----+                                                  |                       
		| 15 |    |    |   |    |              +----+    [22]  [23] [24]  [25]     |
		|----+    | 14 |   | 13 |  [12] [11]   | 10 |    [18]  [19] [20]  [21]     |
		+---------+----+---+----+--------------+----+------------------------------+

	
		01. Audio: pynq.lib.audio
		02. USB
		03. HDMI Out (Display): pynq.lib.video
		04. RPi GPIO: pynq.lib.rpi / pynq.gpio
		05. HDMI In (Camera): pynq.lib.video
		06. Jumper J1 [SD/DSP/JTAG]
		07. Pmod A: pynq.lib.pmod
		08. Pmod B: pynq.lib.pmod
		09. ArduinoShield: pynq.lib.arduino
		10. SD Card Slot
		11. Prgramming Button
		12. Reset	
		13-14. Switch
		15. Power Switch
		16. microUSB
		17. Ethernet
		18-21. Buttons
		22-25. LEDs
		26-27. RGB LEDs
	
	
	1.2 Zynq7000
			#This chip has 2 sides. PL Side and Arm Side
				1. All Rpi Hats Slot, Arduino Shield Slot, Pmod Slot are connected through PL Side
					Library: pynq.gpio
				2. All HDMI In/Out connected to PL
					HDMI Library: pynq.lib.video
				3. USB connected to Arm side of Zynq7000
				4. Audios are connected to PL side of Zynq7000
					Audio Library: pynq.lib.audio
	
	1.3 Starting Up
			1. Connect Ethernet and microUSB to the PC
			2. Ensure Jumper J1 [06] is at SD Mode
			3. Power ON using [15] switch and wait for 4 LEDs (22, 23, 24, 25) to light up
			4. In Windows, go to: Control Panel > Network & Internet > Network & Sharing > Ethernet > Properties > IPv4 > Properties
				> Change "Auto Obtain" to "Manual"
					IP Address: 192.168.2.10
					Subnet Mask: 255.255.255.0
					DNS: 127.0.0.1
			5. Open Google Chrome and go to "http://192.168.2.99:9090/
				Password: xilinx
			6. Click on New > Open Terminal
				Key in: #ifconfig
	
			7. Other Script in Python
				Python 3
					!lscpu = CPU information
					!free-m = RAM information
					!ip-br address show eth0 = Wired Network information
					!hostname = show hostname of board. Instead of using URL, use shortcut. http://{hostname}
					!lsb_release -a = Show Linux version
			8. Python script to check version
				pythonscript.py
					import.sys
					print (f'Python Version: {sys.version}')
					print ('Python path settings:')
					for path_entry in sys.path:
						print(path_entry)
					!pip list --format=columns    //This lists down all installed packages
	
	1.4 Revert Board to Factory settings
			Vivado Board Setup
				1. Download Pynq-Z2 board library, boot images and Master XDC from http://www.tulembedded.com/fpga/ProductsPYNQ-Z2.html
				2. Cope/Move "C:\downloads\boards\TUL\pynq-z2\" to "C:\Xilinx\Pynq\pynq-z2\"
				3. Add initialization file in "Users\user\AppData\Roaming\Xilinx\Vivado\"
					init.tcl
						set_param board.repoPaths [list "C:\Xilinx\Pynq\pynq-z2\A.0"]
				4. Start Vivado > Create Project 
					While choosing board in "board" tab, click on refresh
	
			Build PYNQ SD card image
				https://pynq.readthedocs.io/en/latest/pynq_sd_card.html
				Pynq Z2 version: v3.0.1
				Recommended Ubuntu Version: 20.04
		
		
			Load file to Vivado and Program to board
				1. Start Vivado
				2. Tools > Run Tcl Script > Select Tcl File
				
	1.5 Access the terminal of PYNQ
			Download and install terminal programs
				TeraTerm for Windows
				gtkterm for Ubuntu
			Connect your PYNQ board with USB
			Open TeraTerm/Gtkterm
				Set PYNQ Board to serial port
				Set Baudrate to 115200
				Click Connect and this will lead to root on the terminal
			Install any Package or Library on PYNQ
				sudo pip3.6 install git+https://github.com/Xilinx/BNN-PYNQ.git //For PYNQ v2.0 or later
	1.6 Jupyter Notebook
			Jupyter Header
				Set Notebook to "Markdown"
				#Header Title         //This will create largest font available, with bold
				*Sub header*          //This will be in Italic
				**Subject Header**    //Bold
				*Bullet Points        //Just give the bullet points
				<html>Lilnks</html>   //Provide the link

			Check Python Version in Jupyter	
				from platform import python_version
				print(python_version())
				
			Check Tensorflow Version
				import tensorflow as tf
				print tf.VERSION

			PYNQ Libraries for button and led modules
				Sleep-              //to add delay in the execution of program
				Rgbled_position-
				On-                 //LED on
				Off-                //LED off
				Buttons-            //controls the onboard push button
				Read-               //read the current values of the button
				Toggle-             //Checks the selected elements for visibility
				Wait_for_value-     //Wait for the button to be pressed of released
				
			Programming onboard peripherals
				from pynq.overlays.base import BaseOverlay
				base = BaseOverlay("base.bit")
				from pynq.lib import LED, Switch, Button
				led0 = LED(base.leds[0])
				led0.on()            //use "led0.off()" to off
				
				#Toggle led0 using the sleep() method from time package for it to flash
				import time
				led0 = LED(base.leds[0])
				for i in range(20):
					led0.toggle()
					time.sleep(.1)
					
			PYNQ Libraries for audio modules
				Record()-            //Record data from audio controller to audio buffer
				Save()-              //Save audio buffer content to file 
				Load()-              //Load files into the internal audio buffer
				Play()-              //Play audio buffer via audio jack
				
			Example Code for Audio
				from pynq.overlays.base import BaseOverlay
				base = BaseOverlay("base.bit")
				audio = base.audio
				
				#Record a sample
				audio.record(4)      //Recording 4 seconds of audio
				
				#Save recorded sample
				audio.save("Recording_1.pdm")
				
				#Play recorded sample
				audio.play()
				
				#Load and play audio
				audio.load("/home/xilinx/jupyter_notebooks/base/audio/Recording_1.pdm")
				audio.play()
				
			PYNQ Libraries for video modules
				Configure()-         //Configure the pipeline to use the specified pixel format
				Start()-             //Start the video transfer through the HDMI in
				Tie()-               //Monitor should turn on and show blank screen. To pass the image data through, tie the output to input.
				Numframes-           //Returns a frame of the appropriate size for video mode
				writeframe-          //Write the frame to the video output
				readframe-           //Read the video frame
				Cvtcolour()-         //Converts an image from one color space to another
				
			Example Code for Video
				from pynq import Overlay
				from pynq.drivers.video import HDMI
				
				#Download bitstream
				Overlay("base.bit").download()
				
				#Initialize HDMI as an input device
				hdmi_in = HDMI('in')
				
				##
				hdmi_in.start()
				width = hdmi_in.frame_width()
				height = hdmi_in.frame_height()
				print('HDMI is capturing a video source of resolution {}x{}'\.format(width,height))
			
	1.7 Grove Temperature Sensor v1.2 Example	
			#import libraries and load the overlay-bitstream on FPGA
			from pynq.pl
			import Overlay Overlay("base.bit").download()
			import math
			#instantiating the PMOD for temperature sensor connected at G4
			from pynq.iop import Grove_TMP
			from pynq.iop import PMODB
			from pynq.iop import PMOD_GROVE_G4
			#Read the temperature from the sensor
			tmp = Grove_TMP(PMODB, PMOD_GROVE_G4)
			temperature = tmp.read()
			print(float("{0:.2f}".format(temperature)),'degree Celsius)
	
//////////////////////////////////////////////////////////////////////////////////////////////////
02. PYNQ with Python OpenCV for Image/Video Processing

	2.1 Programming, Conditional Statements, Loops with GPIO
	
		If-else Statement
			from pynq.overlays.base import BaseOverlay
			base = BaseOverlay("base.bit")
			from pynq.lib import LED, Switch, Button
			led0 = LED(base.leds[0])
			switch = Switch(base.switches[0])
			if(switch.read()==1):
				#led0.on()
				led0.on()
			else:
				led0.off()
				
		For Loop Statement
			from pynq.overlays.base import BaseOverlay
			base = BaseOverlay("base.bit")
			from pynq.lib import LED, Switch, Button
			
			#turn ON all LEDs
			for led in base.leds:
				led.on()
				
			#turn ON all LEDs on interval of 1 seconds
			import time
			for led in base.leds:
				led.on()
				time.sleep(1)
				led.off()

		While Loop Statement [Once]
			from pynq.overlays.base import BaseOverlay
			base = BaseOverlay("base.bit")
			from pynq.lib import LED, Switch, Button
			
			led0 = LED(base.leds[0])
			switch = Switch(base.switches[0])
			while(switch.read()==1):
				led0.on()
				
				led0.off()
		
		While Loop Statement [Infinite]
			from pynq.overlays.base import BaseOverlay
			base = BaseOverlay("base.bit")
			from pynq.lib import LED, Switch, Button
			
			led0 = LED(base.leds[0])
			switch = Switch(base.switches[0])
			while True:
				if(switch.read()==1):
					led0.on()
				else:
					led0.off()

		While-else Loop Statement
			from pynq.overlays.base import BaseOverlay
			base = BaseOverlay("base.bit")
			from pynq.lib import LED, Switch, Button
			import time
			
			led0 = LED(base.leds[0])
			switch = Switch(base.switches[0])
			while(switch.read()==1):
				led0.on()
			else:
				led0.off()

		GPIO Control with Loops
			#WHILE IF-ELSE-IF STATEMENT FOR GPIO CONTROL
			from pynq.overlays.base import BaseOverlay
			base = BaseOverlay("base.bit")
			from pynq.lib import LED, Switch, Button
			
			#Turn ON all LEDs
			for led in base.leds:
				led.on()
				
			#CONTROLLING ALL SWITCHES, LED AND BUTTONS
			#Set the number of LED, Switches and buttons
			MAX_LEDS = 4
			MAX_SWITCHES = 2
			MAX_BUTTONS = 4
			leds = [LED(base.leds[index]) for index in range(MAX_LEDS)]
			switches = [Switch(base.switches[index]) for index in range(MAX_SWITCHES)]
			buttons = [Button(base.buttons[index]) for index in range(MAX_BUTTONS)]
			
			#Create lists for each of the IO component groups
			for i in range(MAX_LEDS):
				leds[i] = LED(base.leds[i])
			for i in range(MAX_SWITCHES):
				switches[i] = Switch(base.switches[i])
			for i in range(MAX_BUTTONS):
				buttons[i] = Button(base.buttons[i])
			
			#CHANGING THE CORRESPONDING LED AS THE SWITCH VALUE
			#LEDs start in the off state
			for i in range(MAX_LEDS):
				leds[i].off()
				
			#If a slide switch is on, light the corresponding LED
			for i in range(MAX_LEDS):
				if switches[i%2].read():
					leds[i].on()
				else:
					leds[i].off()

			#IF THE PUSH BUTTONS IS PRESSED AND THE FOLLOWING CODE RUN, THEN THE CORRESPONDING LED WILL TOGGLE 
			for i in range(MAX_LEDS):
				if buttons[i].read():
					leds[i].toggle()
					
					
		SciPy Library
			from scipy import fftpack
			
			A = fftpack.fft(a)
			frequency = fftpack.fftfreq(len(a)) * fre_samp
			figure, axis = plt.subplots()
			
			axis.stem(frequency, np.abs(A))
			axis.set_xlabel('Frequency in Hz')
			axis.set_ylabel('Frequency Spectrum Magnitude')
			axis.set+xlim(-fre_samp / 2, fre_samp / 2)
			axis.set_ylim(-5, 110)
			plt.show()

		SciPy for Image Processing
			#scipy.ndimage is a submodule of SciPy which is used for performing image related operation
			#ndimage means the "n" dimensional image
			#SciPy Image Processing provides Geometrics transformation (rotate, crop, flip), image filtering (sharp and denoising), 
			#display image, image segmentation, classification and feature extraction
			#MISC Package in SciPy contains prebuilt images which can be used to perform image manipulation task
			
			#Show Image
				from scipy import misc
				from matplotlib import pyplot as plt
				import numpy as np
				
				#get face image of panda from misc package
				panda = misc.face()
				
				#plot or show image of face
				plt.imshow(  panda  )
				plt.show()
				
			#Flip down image
				#Flip down using scipy misc.face image
				flip_down = np.flipud(misc.face())
				plt.imshow(flip_down)
				plt.show()
				
			#Rotate Image
				from scipy import ndimage, misc
				from matplotlib import pyplot as plt
				panda = misc.face()
				
				#rotation function of scipy for image - image rotated 135 degrees
				panda_rotate = ndimage.rotate(panda, 135)
				plt.imshow(panda_rotate)
				plt.show()
				
		Numpy
			import numpy as np
			print(np.__version__)
			
			#Create a Numpy Array
			#Simplest way to create an array in Numpy is to use Python List
			myPythonList = [1,9,8,3]
			
			numpy_array_from_list = np.array(myPythonList)
			numpy_array_from_list

		Matplotlib
			%matplotlib inline
			from matplotlib import pyplot as plt
			import numpy as np
			
			#Frequency in terms of Hz
			fre = 5
			
			#Sample Rate
			fre_samp = 50
			t = np.linspace(0, 2, 2 * fre_samp, endpoint = False)
			a = np.sin(fre * 2 * np.pi * t)
			figure, axis = plt.subplots()
			axis.plot(t, a)
			axis.set_xlabel ('Time(s)')
			axis.set_ylabel ('Signal amplitude')
			plt.show()


	2.2 OpenCV Development - OpenCV Basics
		
		PIL Library
			Showing an image
				from PIL import Image as PIL_Image
				orig_img_path = ('/home/xilinx/jupyter_notebooks/Digitronix/Pynq.jpg') //Upload an image file to here in Jupyter
				
				#Locate the image directory
				img = PIL_Image.open(orig_img_path)
				
				#show the image
				img
				
			Grey Scale an Image
				from PIL import Image as PIL_Image
				orig_img_path = ('/home/xilinx/jupyter_notebooks/Digitronix/Pynq.jpg') //Upload an image file to here in Jupyter
				
				#Locate the image directory
				img = PIL_Image.open(orig_img_path)
				
				#Gray scale the image
				bw_img = img.convert("L")
				
				#Show the Image
				bw_img

			Rotate the Image
				from PIL import Image as PIL_Image
				orig_img_path = ('/home/xilinx/jupyter_notebooks/Digitronix/Pynq.jpg') //Upload an image file to here in Jupyter
				
				#Locate the image directory
				img = PIL_Image.open(orig_img_path)
				
				#Rotate the Image
				rot_img = img.rotate(45) //45 degrees
				
				#Show image
				rot_img
				
		Matplot Library
			
			Color Representation - Shows both BGR and RGB images side by side
				import cv2
				from matplotlib import pyplot as plt
				img = cv2.imread('/home/xilinx/jupyter_notebooks/Digitronix/Pynq.jpg')
				
				#In OpenCV images are actually represented in BGR order rather than RGB 
				plt.subplot(121), plt.imshow(img)
				
				#All we need to do is convert the image from BGR to RGB 
				plt.subplot(122), plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
			
			Display a Matplotlib RGB Image
				from matplotlib import pyplot as plt
				import matplotlib.image as mpimg
				img = mpimg.imread('/home/xilinx/jupyter_notebooks/Digitronix/Pynq.jpg')
				plt.imshow(img)

			Comparing OpenCV and MpImg images side by side
				import cv2
				import numpy as np
				from matplotlib import pyplot as plt
				import matplotlib.image as mpimg
				
				img = cv2.imread('/home/xilinx/jupyter_notebooks/Digitronix/Pynq.jpg')
				img2 = mpimg.imread('/home/xilinx/jupyter_notebooks/Digitronix/Pynq.jpg')
				
				plt.subplot(2,2,1),plt.imshow(img2)
				plt.title('OpenCV'),
				plt.subplot(2,2,2),plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
				plt.title('mpimg')

		OpenCV Library
			Shifting Image 2
				import cv2
				import numpy as np
				img = cv.imread('/home/xilinx/jupyter_notebook/Digitronix/cat.jpg',0)
				rows,cols = img.shape
				
				M = np.float32([[1,0,100], [0,1,50]])
				dst = cv2.warpAffine(img,M,(cols,rows))
				
				cv2.imshow('img',dst)
				cv2.waitKey(0)
				cv2.destroyAllWindows()		

			Rotating an Image - 90 degree rotation
				img = cv2.imread('/home/xilinx/jupyter_notebook/Digitronix/cat.jpg',0)
				rows,cols = img.shape
				
				M = cv2.getRotationMatrix2D((cols/2, rows/2),90,1)
				dst = cv2.warpAffine(img,M,(cols,rows))


			Geometric Transformation - Isometric View
				img = cv2.imread('/home/xilinx/jupyter_notebook/Digitronix/drawing.jpg')
				rows,cols,ch = img.shape
				
				pts1 = np.float32([[50,50],[200,50],[50,200]])
				pts2 = np.float32([[10,100],[200,50],[100,250]])
				
				M = cv2.getAffineTransform(pts1,pts2)
				dst = cv2.warpAffine(img,M,(cols,rows))
				
				plt.subplot(121),plt.imshow(img),plt.title('Input')
				plt.subplot(122),plt.imshow(dst),plt.title('Output')
				plt.show()

			Perspective Transformation - Make Sudoku photo to align with grid
				img = cv2.imread('/home/xilinx/jupyter_notebook/Digitronix/sudoku.jpg')
				rows,cols,ch = img.shape
				
				pts1 = np.float32([[56,65],[368,52],[28,387],[389,390]])
				pts2 = np.float32([[0,0],[300,0],[0,300],[300,300]])
				
				M = cv2.getPerspectiveTransform(pts1,pts2)
				dst = cv2.warpPerspective(img,M,(300,300))
				
				plt.subplot(121),plt.imshow(img),plt.title('Input')
				plt.subplot(122),plt.imshow(dst),plt.title('Output')
				plt.show()
				
			Smoothing Images
				import cv2
				import numpy as np
				from matplotlib import pyplot as plt
				
				img = cv2.imread('/home/xilinx/jupyter_notebook/Digitronix/opencv_logo.png')
				
				kernel = np.ones((5,5),np.float32)/25
				dst = cv2.filter2D(img,-1,kernel)
				
				plt.subplot(121),plt.imshow(img),plt.title('Original')
				plt.xticks([]), plt.yticks([])
				plt.subplot(122),plt.imshow(dst),plt.title('Averaging')
				plt.xticks([]), plt.yticks([])
				plt.show()
				
			Blurring Images
				import cv2
				import numpy as np
				from matplotlib import pyplot as plt
				
				img = cv2.imread('/home/xilinx/jupyter_notebook/Digitronix/opencv_logo.png')
				
				blur = cv2.blur(img,(5,5))
				
				plt.subplot(121),plt.imshow(img),plt.title('Original')
				plt.xticks([]), plt.yticks([])
				plt.subplot(122),plt.imshow(blur),plt.title('Blurred')
				plt.xticks([]), plt.yticks([])
				plt.show()
				
			Gaussian Blurring Images
				import cv2
				import numpy as np
				from matplotlib import pyplot as plt
				
				img = cv2.imread('/home/xilinx/jupyter_notebook/Digitronix/opencv_logo.png')
				
				blur = cv2.GaussianBlur(img,(5,5),0)
				
				plt.subplot(121),plt.imshow(img),plt.title('Original')
				plt.xticks([]), plt.yticks([])
				plt.subplot(122),plt.imshow(blur),plt.title('Blurred')
				plt.xticks([]), plt.yticks([])
				plt.show()
				
			Median Filtering - Lowering down the image noise
				import cv2
				import numpy as np
				from matplotlib import pyplot as plt
				
				img = cv2.imread('/home/xilinx/jupyter_notebook/Digitronix/opencv_logo.png')
				
				median = cv2.medianBlur(img,5)
				
				plt.subplot(121),plt.imshow(img),plt.title('Original')
				plt.xticks([]), plt.yticks([])
				plt.subplot(122),plt.imshow(median),plt.title('Median')
				plt.xticks([]), plt.yticks([])
				plt.show()

			Bilateral Filtering - Blurring without blurring the edges
				import cv2
				import numpy as np
				from matplotlib import pyplot as plt
				
				img = cv2.imread('/home/xilinx/jupyter_notebook/Digitronix/opencv_logo.png')
				
				blur = cv2.bilateralFilter(img,9,75,75)
				
				plt.subplot(121),plt.imshow(img),plt.title('Original')
				plt.xticks([]), plt.yticks([])
				plt.subplot(122),plt.imshow(blur),plt.title('Blurred')
				plt.xticks([]), plt.yticks([])
				plt.show()

	2.3 Face Eye Detection - Morphological Tranformations
		
		OpenCV Library
			Morphological Transformations
				import cv2
				import numpy as np
				img = cv2.imread('/home/xilinx/jupyter_notebook/Digitronix/j.png',0)
				kernel = np.ones((5,5),np.uint8)
				erosion = cv2.erode(img,kernel,iterations = 1)
				dilation = cv2.dilate(img,kernel,iterations = 1)
				
				opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)
				closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)
				gradient = cv2.morphologyEx(img, cv2.MORPH_GRADIENT, kernel)
				tophat = cv2.morphologyEx(img, cv2.MORPH_TOPHAT, kernel)
				blackhat = cv2.morphologyEx(img, cv2.MORPH_BLACKHAT, kernel)
				
			Image Gradients
				import cv2
				import numpy as np
				from matplotlib import pyplot as plt
				img = cv2.imread('/home/xilinx/jupyter_notebook/Digitronix/sudoku.jpg',0)
				laplacian = cv2.Laplacian(img, cv2.CV_64F)      //Inverse image, bold lines and thin lines become equal
				sobelx = cv2.Sobel(img,cv2.CV_64F,1,0,ksize=5)  //Inverse Image, only vertical line become visible
				sobely = cv2.Sobel(img,cv2.CV_64F,0,1,ksize=5)  //Inverse image, only horizontal line become visible
				
				#Show Image 1
				plt.imshow(img,cmap = 'gray')
				
				#Show Image 2
				#Original Image
				plt.subplot(2,2,1),plt.imshow(img,cmap = 'gray')
				plt.title('Original'), plt.xticks([]), plt.yticks([])
				#showing the sobel X output
				plt.subplot(2,2,2),plt.imshow(sobelx,cmap = 'gray')
				plt.title('SobelX'), plt.xticks([]), plt.yticks([])
				
			Sobel Edge Detection
				import cv2
				import numpy as np
				from matplotlib import pyplot as plt
				
				img0 = cv2.imread('/home/xilinx/jupyter_notebook/Digitronix/sudoku.jpg')  //Or use Pynq.jpg image
				
				gray = cv2.cvtColor(img0, cv2.COLOR_BGR2GRAY)
				img = cv2.GaussianBlur(gray,(3,3),0)
				
				sobelx = cv2.Sobel(img,cv2.CV_64F,1,0,ksize=5)  //x
				sobely = cv2.Sobel(img,cv2.CV_64F,0,1,ksize=5)  //y
				
				plt.subplot(2,2,1),plt.imshow(img,cmap = 'gray')
				plt.title('Original'), plt.xticks([]), plt.yticks([])
				
				plt.subplot(2,2,2),plt.imshow(img,cmap = 'gray')
				plt.title('SobelX'), plt.xticks([]), plt.yticks([])
				plt.subplot(2,2,3),plt.imshow(img,cmap = 'gray')
				plt.title('SobelY'), plt.xticks([]), plt.yticks([])
				
				plt.show


			Canny Edge Detection
				import cv2
				import numpy as np
				from matplotlib import pyplot as plt
				img0 = cv2.imread('/home/xilinx/jupyter_notebook/Digitronix/sudoku.jpg',0)
				edges = cv2.Canny(img,100,200)
				
				plt.subplot(121),plt.imshow(img,cmap = 'gray')
				plt.title('Original'), plt.xticks([]), plt.yticks([])
				plt.subplot(122),plt.imshow(img,cmap = 'gray')
				plt.title('Edge Image'), plt.xticks([]), plt.yticks([])
				
				plt.show
			
			Harris Corner Detection - Find corners and mark as green lines
				import cv2
				import numpy as np
				img = cv2.imread('/home/xilinx/jupyter_notebook/Digitronix/sudoku.jpg')
				
				gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
				gray = np.float32(gray)
				dst = cv2.cornerHarris(gray,2,3,0.04)
				
				#result is dilated for marking the corners, not important
				dst = cv2.dilate(dst,None)
				
				#Threshold for an optimal value, varies depending on the image
				img[dst>0.01*dst.max()]=[0,0,255]
				cv2.imshow('dst',img)
				if cv2.waitKey(0) & 0xff == 27:
					cv2.destroyAllWindows()
			
			
			FAST Algorithm for Corner Detection
				Feature Detection using FAST
					1. Select a pixel "p" in the image which is to be identified as an intereset point or not. Let intensity be "Ip"
					2. Select appropriate threshold value "t"
					3. Consider a circle of 16 pixels around the pixel under test
				Machine Learning
					1. Select a set of images for training (preferably from the target application domain)
					2. Run FAST algorithm in every images to find feature points
					3. For every feature point, store the 16 pixels around it as a vector. Do it for all the images to get feature vector "p"
					4. Each pixel (say x) in these 16 pixels can have one of the following three states: a corner detector
				Non-Maximal Suppression - Detecting multiple interest points in adjacent locations is a problem. Solved by using Non-Maximum Suppression.
					1. Compute a score function, "V" for all the detected feature points. 
						"V" is the sum of absolute difference between "p" and 16 surrounding pixels values
					2. Consider two adjacent keypoints and compute their "V" values
					3. Discard the one with lower "V" value
					
			
			FAST Algorithm Example
				import numpy as np
				import cv2 as cv
				from matplotlib import pyplot as plt
				img = cv.imread('blox.jpg', cv.IMREAD_GRAYSCALE) # `<opencv_root>/samples/data/blox.jpg`
				
				# Initiate FAST object with default values
				fast = cv.FastFeatureDetector_create()
				
				# find and draw the keypoints
				kp = fast.detect(img,None)
				img2 = cv.drawKeypoints(img, kp, None, color=(255,0,0))
				
				# Print all default params
				print( "Threshold: {}".format(fast.getThreshold()) )
				print( "nonmaxSuppression:{}".format(fast.getNonmaxSuppression()) )
				print( "neighborhood: {}".format(fast.getType()) )
				print( "Total Keypoints with nonmaxSuppression: {}".format(len(kp)) )
				cv.imwrite('fast_true.png', img2)
				
				# Disable nonmaxSuppression
				fast.setNonmaxSuppression(0)
				kp = fast.detect(img, None)
				print( "Total Keypoints without nonmaxSuppression: {}".format(len(kp)) )
				img3 = cv.drawKeypoints(img, kp, None, color=(255,0,0))
				cv.imwrite('fast_false.png', img3)
			
		Face & Eye Detection
			Classifiers - Face Detection
				#Already been trained to detect faces using millions of images to get accuracy
				#There are two types of classifiers in OpenCV: Local Binary Pattern (LBP), and Haar Cascades
				#Haarcascade face detection uses training data called: haarcascade_frontalface_default.xml
				#Haarcascade eye detection uses training data called: haarcascade_eye.xml
				
				import cv2
				import numpy as np
				from matplotlib import pyplot as plt
				img = cv2.imread('/home/xilinx/jupyter_notebook/Digitronix/faces.jpg')
				cascPath = "/home/xilinx/jupyter_notebook/base/video/data/haarcascade_frontalface_default.xml
				
				#Create the haarcascade 
				faceCascade = cv2.CascadeClassFilter(cascPath)
				gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
				
				#Detect faces in the image
				faces = faceCascade.detectMultiScale(
					gray,
					scaleFactor=1.1,
					minNeighbors=5,
					minSize=(30,30)
					#flags = cv2.cv.CV_HAAR_SCALE_IMAGE
				)
				
				print("Found {0} faces!".format(len(faces)))
				
				#Draw a rectangle around the face
				for (x,y,w,h) in faces:
					cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)
				plt.imshow(image)	
				#cv2.imshow("Faces found", image)
				
			Classifiers - Face Color Detection
				import cv2
				import numpy as np
				from matplotlib import pyplot as plt
				img = cv2.imread('/home/xilinx/jupyter_notebook/Digitronix/faces.jpg')
				
				np_frame = image
				gray = cv2Color(np_frame, cv2.COLOR_BGR2GRAY)
				
				#Haar Classifiers
				face_cascade = cv2.CascadeClassifier(
					'home/xilinx/jupyter_notebook/base/video/data/'
					'haarcascade_frontalface_default.xml'
				)
				faces = face_cascade.detectMultiScale(gray, 1.3, 5)          //Kernel size or size of image reduced, when detection being applied
				for (x,y,w,h) in faces:                                      //5 is the number of neighbors after which we accept that is face
					cv2.rectangle(np_frame,(x,y),(x+w,y+h),(255,0,0),2)
					
				#Output OpenCV results via matplotlib 
				plt.imshow(image[:,:,[2,1,0]])
				plt.show()
			
			FACE and EYE Detection with Haar Cascade 
				import cv2
				import numpy as np
				from matplotlib import pyplot as plt
				img = cv2.imread('/home/xilinx/jupyter_notebook/Digitronix/one_face.jpg')
				np_frame=image 
				
				gray = cv2.cvtColor(np_frame, cv2.COLOR_BGR2GRAY)
				
				#haar classifiers
				face_cascade = cv2.CascadeClassifier(
					'home/xilinx/jupyter_notebook/base/video/data/'
					'haarcascade_frontalface_default.xml'
				)
				
				face_cascade = cv2.CascadeClassifier(
					'home/xilinx/jupyter_notebook/base/video/data/'
					'haarcascade_eye.xml'
				)
				
				gray = cv2.cvtColor(np_frame, cv2.COLOR_BGR2GRAY)
				faces = face_cascade.detectMultiScale(gray, 1.3, 5)
				
				#Iterate over the faces and detect eyes
				for (x,y,w,h) in faces:
					cv2.rectangle(np_frame, (x,y), (x+w,y+h), (255,0,0),2)
					
					#Arguments => image, top left coord, bottom right coord, color, rectangle border thickness
					#Need two regions of interest(ROI)grey and color for eyes to detect and another to draw the rect
					roi_gray = gray[y:y+h, x:x+w]
					roi_color = np_frame[y:y+h, x:x+w]
					
					#Detecting eyes
					eyes = eye_cascade.detectMultiScale(roi_gray)
					
					#Draw rectangle over eyes
					for (ex,ey,ew,eh) in eyes:
						cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh), (0,255,0),2)
			
				plt.imshow(image[:,:,[2,1,0]])
				plt.show()
				
	2.4 HDMI Streaming and Processing
		//https://community.element14.com/products/roadtest/rv/roadtest_reviews/689/pynq-z2_dev_board_py_3
		
		Streaming/Processing directly from HDMI Input
			#Board Setup
			1. Connect PYNQ's HDMI Out to Display
			2. Connect PYNQ's HDMI In to Laptop
			3. Connect PYNQ's Ethernet to Laptop
			4. Connect PYNQ's Power to USB
			5. Run 5_base_overlay_video.ipynb
			
	2.5 License Plate Localizer	
		1. Download license_plate_localization_pynq.ipynb
		2. Upload to Jupyter Notebook
			
//////////////////////////////////////////////////////////////////////////////////////////////////
03. Installing Pyhton Library in PYNQ - Cryptography Library
		github.com/amiralis/python-crypto-tutorial
	
	Two methods:
		1. Connecting PYNQ with PROG-UART Port with USB cable and using the serial terminal programs.
			TerraTerm / Putty (Baud Rate of 115200)
		2. Connecting PYNQ with LAN/Ethernet cable and accessing with Jupyter's Terminal Console
			In Jupyter, click on "New" and select Terminal
		
	Jupyter Terminal:
		pip3 install cryptography
		pip3 install pycryptodome
		
	Retrieving the installed library list on Jupyter
		import pip        //needed to use the pip functions
		for i in pip.get_installed_distributions(local_only=True):
			print(i)
			
	Git-clone or Upload project to Pynq
		In Jupyter, click on "Upload"
		

//////////////////////////////////////////////////////////////////////////////////////////////////
04. Machine Learning with Python in PYNQ
	
	Libraries already available in PYNQ 
		CNN, //github.com/awai54st/PYNQ-Classification
		QNN, //github.com/Xilinx/QNN-MO-PYNQ
		BNN  //github.com/Xilinx/BNN-PYNQ
	
	IMPORTANT LINKS
	//https://pynq.readthedocs.io/en/v2.0/overlay_design_methodology/python_packaging.html
	//https://github.com/hillhao/PYNQ-project#5-neural-network-design

	4.1 Character Recognition with BNN - Vehicle Number Plate Detection
		
		Installation
			Open Jupyter and open a Terminal
			sudo pip3 install git+https://github.com/Xilinx/BNN-PYNQ.git      //On PYNQ v2.3 and later versions, tested up to v2.5
			sudo pip3.6 install git+https://github.com/Xilinx/BNN-PYNQ.git    //On PYNQ v2.2 and earlier

		Major steps on "Character Recognition System"
			Load the overlay and BNN
			Download the network parameters
			Load image from camera
			Image segmentation

		Char_Recognition_with_PYNQ_V2.ipynb

//////////////////////////////////////////////////////////////////////////////////////////////////
05. Creating Custom Overlay (VIVADO Project) for PYNQ
	//pynq.readthedocs.io/en/v2.4/overlay_design_methodology/overlay_tutorial.html

	/////////////////////////////////
	5.1 Custom Overlay: Addition and Multiplication App
		
		Components required in th eprocess of creating an overlay
			Board settings
			PS-PL Interface
			MicroBlaze Soft Processors
			Python/C Integration
			Python AsyncIO
			Python Overlay API
			Python Packaging
			
		Addition & Multiplication based overlay with Vivado HLS, IP Integrator and Jupyter
			Create custom Multiplier IP with Vivado HLS and implement it on Jupyter Interface
			Create Vivado HLS IP with C++ for the add and mul of two numbers.
			Synthesize and export RTL/IP of this IP to Vivado IP Integrator
			In Vivado IP integrator, 
				Create Block Design with Zynq PS and HLS IP.
				Run the Automation
				Create HDL Wrapper
				Synthesize the design
				Implement the Design
				Generate the Design
		
		Vivado HLS Source Code for Adder and Multiplicator [adder_multiplier.cpp]
			void addmul(int a, int b, int& c, int& m){
			#pragma HLS INTERFACE ap_ctrl_none port=return
			#pragma HLS INTERFACE s_axilite port=a
			#pragma HLS INTERFACE s_axilite port=b
			#pragma HLS INTERFACE s_axilite port=c
			#pragma HLS INTERFACE s_axilite port=m
			
				c = a + b;
				m = a * b;
				
			}
			
		Vivado HLS 2018 [Step 1]
			01. Create new project, name "Project Name" as "addmul_hls" and click "Next"
			02. Create "New File", navigate to "addmul" folder and save file as "addmul.cpp". "addmul.cpp" will appear in the panel.
			03. Key in "addmul" to "Top Functions" textbox and click Next
			04. Skip "TestBech Files" selection window by clicking "Next"
			05. In Solution Configuration window, under "Part Selection" section, click on "browse icon" and select xc7z020clg400-1 for PYNQ Z2 FPGA
			06. Click on OK to close the Part Selection window and click on Finish
			07. You will now see project window on Vivado HLS
			08. Expand the "Sources" option on left Pane "Explorer"
			09. Double click on "addmul.cpp"
			10. Copy Paste the source code above to this "addmul.cpp" and save the project
			11. Click on "Run C Synthesis". You will get Synthesis report once the synthesis completes.
			12. Click on "Export RTL" [The icon with 4 section box]
			13. Leave Format Selectrion as default [IP Catalog]
			14. Under "Evaluate Generated RTL" section, select Verilog or VHDL language and click OK //VHDL being used here
			15. After IP exports completes, open Vivado tool from startup menu
			16. Visit Explorer Pane > addmul_hls > solution1 > impl > ip > drivers > addmul_v1_0 > src > xaddmul_hw.haar
					Here, you can see the "XADDMUL_AXILITES_ADDR_A_DATA" 0x10, etc, etc
					All these ar in line with the Source code
	
		Vivado IP Integration [Step 2]
			01. Open Vivado [NOT Vivado HLS] from startup menu
			02. Create new project on Vivado
			03. Click on "RTL Project" and click Next [Ensure "do not specify sources" checkbox is checked]
			04. Select Part/Board
			05. In "Part Selection", select xc7z020clg400-1 for PYNQ FPGA. Click Next and "Finish"
			06. Vivado Project window will appear
			07. In Left window Pane, Under IP Integrator, click on "Create Block Design", use default name "design_1" and click OK
					Design Name: design_1
					Directory: <Local to Project>
					Specify source set: <Design Sources>
			08. In Left window Pane, Under Project Manager, click on "Settings 
					Project Setting > IP > Repository 
						Click on "+" button
						Highlight the "ip" folder and click on "Select"
							pynq_overlay/addmul/addmul_hls/solution1/impl/ip
						Vivado will automatically sync the Ip
					Click on "Apply"
			09. In "Block Design" window, in Diagram section, click on "+" button:
					Search for addmul and add into Diagram. "addmul_0" block will appear
					In Diagram toolbar, click on "+". Search for "Zynq Processing System" and add this block
					Click on Run Block Automation
						Make sure "processing_system7_0" checkbox is checked
						Options:
							Make Interface External: FIXED_IO, DDR
							Cross Trigger In: Disable
							Cross Trigger Out: Disable
						Click OK
					Click on Run Connection Automation
						Make sure "addmul_0" and "s_axi_AXILiteS" checkboxes are checked 
						Options:
							Master: /processing_system7_0/M_AXI_GP0 [greyed out]
							Bridge IP: New AXI Interconnect [greyed out]
							Clock source for driving Interconnected IP: Auto
							Clock source for Master Interface: Auto
							Clock source for Slave Interface: Auto
						Click on OK
					In Diagram Toolbar, click on "Box with Tick" icon to validate the design
					Open Address Editor Tab to check the addresses
	
		Vivado Project Generation [Step 3]
			01. You will see the Block Design window appears with blocks interconnected in the Diagram Tab
			02. On the Top Left Pane of "Block Design" window, Goto "Sources" tab and right click on Design Sources > design_1 [mentioned in Step 2]
					Notice the yellow icon
			03. Click on "Create HDL Wrapper"
			04. Click on "Let VIVADO manage wrapper and auto update" option and press OK
					Now we have the HDL of the block design we made.
					Notice that the Yellow icon turns into Purple icon and name turns into "design_1-wrapper"
			05. In Flow Navigator > Program and Debug > Generate Bitstream
					Click on "Generate Bitstream"
						Launch Directory: <Default Launch Directory>
						Checked Launch runs on local host: 
						Number of Jobs: 4
						Unchecked Generate scripts only
						Click OK
					Bitstream Generation Completed window appears
						Select "Cancel"
			06. Goto File > Export and click on Export Block Design 
					Export Block Design
						Tcl file: ../Desktop/pynq_overlay/addmul/addmul_vivado/design_1.tcl
						Make sure "Automatically create top design" checkbox is checked.
						Click OK
			07. Copy the BitStream (../Desktop/pynq_overlay/addmul/addmul_vivado/addmul_vivado.runs/impl_1/design_1_wrapper.bit) 
			08. Paste the Bitstream file together in the folder containing the Tcl file (../Desktop/pynq_overlay/addmul/addmul_vivado/design_1.tcl). 
			09. Name these two files using the same name (design_1_addmul.tcl, design_1_addmul.bit)
			10. Copy Paste both these BitStram file and Tcl file into PYNQ FPGA file directory:
					Goto file explorer on windows, type: \\192.168.2.99\9090 or \\pynq\xilinx
					Enter username and password: xilinx
					Navigate to Network\pynq\xilinx\jupyter_notebooks\
					Create new folder "addmul" and add the 2 files in the folder
					--OR--
					Upload directly from Jupyter Interface in Chrome
			11. Goto Jupyter Interface
			12. Create New Python 3 file
	
		Python Program in Jupyter
			from pynq import Overlay
			overlay = Overlay('/home/xilinx/jupyter_notebooks/addmul/design_1_addmul.bit')
			
			overlay?
			
			add_ip = overlay.addmul_0
			
			#give input a=4
			add_ip.write(0x10, 4)
			#give input b=5
			add_ip.write(0x18, 5)
			
			#read the addition result
			add_ip.read(0x20)    //Output: 9 
			
			#read the multiplication result
			ass_ip.read(0x28)    //Output20
		
		Creating a Driver
			from pynq import DefaultIP
			class AddDriver(DefaultIP):
				def __init__(self, description):
					super().__init__(description=description)
				
				bindto = ['xilinx.com:hls:addmul:1.0'] 
				//Verify the version in Vivado [Explorer Pane > addmul_hls > solution1 >impl > ip > drivers > addmulv1_0]
				
				def add(self, a, b):
					self.write(0x10, a)
					self.write(0x18, b)
					return self.read(0x20)
					
				def mul(self, a, b):
					self.write(0x10, a)
					self.write(0x18, b)
					return self.read(0x28)
			
			overlay = Overlay('/home/xilinx/jupyter_notebooks/addmul/design_1_addmul.bit')
			
			overlay.addmul_0.add(15,20)    //Output:35
			
			overlay.addmul_0.mul(15,2)     //Output:30

	/////////////////////////////////
	5.2 How to create a Custom Overlay

		/////////////////////////////////
		Creating basic GPIO Overlay (Vivado Design with GPIO Controlling)
			Download HDL code of Johnson Counter from myhdl.org
				https://www.myhdl.org/docs/examples/jc2.html
			Also, Verilog code for Clock Divider from AllaboutFPGA
			
			Create New Project in Vivado
				Project name: Johnson_counter
				Location: c:/johnson_counter
				Create subdir: Checkbox checked
				Click on Next
				
				Project Type window
					Check RTL Project checkbox
					Do not specify sources at this time: checkbox checked
					Click Next
					
				Default Part Windows
					Select "Boards" Tab
					Select PYNQ Board
					Click on Next
					
				Click on Finish
				
			Flow Navigator Panel > IP Integrator > Create Block Design
				Create Block Design window
					Design Name: design_1
					Directory: <Local to Project>
					Specify source set: Design Sources
					Click OK
					
			Block Design Panel > Diagram window	
				Click on "+" on "This design is empty. Press the + button"
				Search for "zynq7 processing system" and press enter
				A block will appear.
				Click on "Run Block Automation"
				Run Block Automation Window
					Make sure check boxes on the left panel are checked
					Options:
						Make Internal External: FIXED_IO, DDR
						Apply Board Preset: Checkbox checked
						Cross Trigger In: Disable
						Cross Trigger Out: Disable
						Click on OK
			
			Go to MyHDL.org and copy a snippet from Johnson Counter. Link above.
				module jc2 (
					goLeft,
					goRight,
					stop,
					clk,
					q
				);
				// A bi-directional 4-bit Johnson counter with stop control.
				// 
				// I/O pins:
				// --------
				// clk      : input free-running slow clock 
				// goLeft   : input signal to shift left (active-low switch)
				// goRight    : input signal to shift right (active-low switch)
				// stop     : input signal to stop counting (active-low switch)
				// q        : 4-bit counter output (active-low LEDs; q[0] is right-most)

				input goLeft;
				input goRight;
				input stop;
				input clk;
				output [3:0] q;
				reg [3:0] q;

				reg [0:0] dir;
				reg run;



				always @(posedge clk) begin: JC2_LOGIC
					if ((goRight == 0)) begin
						dir <= 1'b0;
						run <= 1'b1;
					end
					else if ((goLeft == 0)) begin
						dir <= 1'b1;
						run <= 1'b1;
					end
					if ((stop == 0)) begin
						run <= 1'b0;
					end
					if (run) begin
						if ((dir == 1'b1)) begin
							q[4-1:1] <= q[3-1:0];
							q[0] <= (!q[3]);
						end
						else begin
							q[3-1:0] <= q[4-1:1];
							q[3] <= (!q[0]);
						end
					end
				end

				endmodule
			
			In Flow Navigator, click on "IP Integrator" to open Block Design window 
				Block Design window 
					Open Sources Tab and right click to select "Add sources"
					Add Sources Window
						Select Add or create design sources
						Click on Next
						
						Add or create Design sources window
							Click on "Create File"
							
							Create Source File window
								File Type: Verilog
								File name: johnson_counter
								File location: <Local to Project>
								Click on OK
								
							Click on Finish	
						
						Define Module wnidow will appear
							Module name: johnson_counter
							I/O Port Definitions: Leave blank
							Click OK and save.
							
					johnson_counter.v will appear under Design Sources directory in the Sources tab
						Double click to open it
						Paste the copied code into this space and save file
						
					Open Diagram tab
						Drag and drop "jc2(johnson_counter.v)" from Sources tab
						Right click on "q[3:0]" node and select "make external"
						A node will appear "q_0[3:0]"
						Click on this external node "q_0[3:0]" and navigate to "External Port Properties" panel on the left
							Name: leds
							Direction: Output
							From: 3 To: 0
							Net: jc2_0_q
							
						Double click on Zynq7 Processing System Block
							ZYNQ7 Processing System Window
								Page Navigator > MIO Configuration > IO Peripherals > GPIO
									Check EMIO GPIO (Width)
									Click on OK
						
				Clockrate, visit https://allaboutfpga.com/vhdl-code-for-clock-divider/		
				Copy VHDL Code for Clock Divider
					library IEEE;
					use IEEE.STD_LOGIC_1164.ALL;
					use IEEE.numeric_std.ALL;
					  
					entity Clock_Divider is
					port ( clk,reset: in std_logic;
					clock_out: out std_logic);
					end Clock_Divider;
					  
					architecture bhv of Clock_Divider is
					  
					signal count: integer:=1;
					signal tmp : std_logic := '0';
					  
					begin
					  
					process(clk,reset)
					begin
					if(reset='0') then  
					count<=1;
					tmp<='0';
					elsif(clk'event and clk='1') then
					count <=count+1;
					if (count = 25000) then
					tmp <= NOT tmp;
					count <= 1;
					end if;
					end if;
					clock_out <= tmp;
					end process;
					  
					end bhv;
					
				Back in Vivado > IP Integrator > Block Design 
					Open "Sources" tab
					Right click and click on "Add sources"
					Click on "Add or create design sources" and click Next
					Click on Create File
						File type: VHDL
						File name: clock_divider
						File Location: <Local to Project>
						Click OK
						Click on Finish
						
					Define Module Window
						Entity name: clock_divider
						Architecture: Behavioral
						Leave IO Port Definition blank
						Click OK and use values
						clock_divider.vhd will appear in the Design sources directory
						Open "clock_divider.vhd" text editor
						Paste clock divider VHDL code from above and save. 
						The file "clock_divider(Behavioral)(clock_divider.vhd)" will be automatically renamed to "Clock_Divider(bhv)(clock_divider.vhd)"
					
					Open Diagram Tab						
						Drag and drop "Clock_Divider(bhv)(clock_divider.vhd)" into Block Diagram panel
						Link up "processing_system7_0 > FCLK_CLK0" to "Clock_Divider_0 > clk"
						Link up "processing_system7_0 > FCLK_RESET0_N" to "Clock_Divider_0 > reset"
						Link up "Clock_Divider_0 > clock_out" to "jc2_0 > clk"
						Expand "processing_system7_0 > GPIO_0"
						Add IP to the block diagram and search for "Slice". This is to select specific GPIO wire from the 64bit bundle
						Link "xlslice_0 > Din[31:0]" to "processing_system7_0 > GPIO_0 > GPIO_O[63:0]"
						Click on "xlslice_0" block and under Block Properties panel, rename to "left"
						Duplicate this block and rename to "right"
						Duplicate again and rename this block as "stop"
						Jump the "right > Din[31:0]", "stop > Din[31:0]", and "left > Din[31:0]" together
						Link "right > Dout[0:0]" to "jc2_0 > goRight"
						Link "stop > Dout[0:0]" to "jc2_0 > stop"
						Link "left > Dout[0:0]" to "jc2_0 > goLeft"
						Double click on "left" block and Re-customize window will pop up
							Din Width: 32
							Din From: 0     //left will be controlled by GPIO 0
							Din Down To: 0
							Dout Width: 1
						Double click on "right" block and Re-customize window will pop up
							Din Width: 32
							Din From: 1     //right will be controlled by GPIO 1
							Din Down To: 1
							Dout Width: 1
						Double click on "stop" block and Re-customize window will pop up
							Din Width: 32
							Din From: 2     //stop will be controlled by GPIO 2
							Din Down To: 2
							Dout Width: 1
						Double click on "processing_system7_0" block and Re-customize window will pop up
							PS-PL Configuration Tab
								Expand AXI Non Secure Enablement > GP Master AXI Interface
									Uncheck box for "M AXI GP0 interface"
									Click on OK
						Click on "Validate design"	
					
					Right click on "design_1" under Sources > Design Sources
						Select "Create HDL wrapper"
							Choose "Let Vivado manage wrapper and auto-update"
							Click OK and ignore warnings
								Warnings are because not all GPIOs are connected
								
					Right click on "Sources Tab > Design Sources > design_1_wrapper" and select "Set as top"
					
			Back in Vivado, Flow Navigator > Synthesis
				Click on "Run Synthesis"
				Synthesis Completed window
					Select "Open Synthesized Design
					Click OK
			
			If Synthesized Design Panel does not show the checkered Package screen, activate the I/O Planning
				Top menu bar > Layout > I/O Planning
			
			Download "Master XDC file" from Pynq website and open the xdc file in Notepad++
				Search for "LEDs" and you'll find a list of codes under this header*
					#set_property -dict { PACKAGE_PIN R14   IOSTANDARD LVCMOS33 } [get_ports { led[0] }]; #IO_L6N_T0_VREF_34 Sch=led[0]
					#set_property -dict { PACKAGE_PIN P14   IOSTANDARD LVCMOS33 } [get_ports { led[1] }]; #IO_L6P_T0_34 Sch=led[1]
					#set_property -dict { PACKAGE_PIN N16   IOSTANDARD LVCMOS33 } [get_ports { led[2] }]; #IO_L21N_T3_DQS_AD14N_35 Sch=led[2]
					#set_property -dict { PACKAGE_PIN M14   IOSTANDARD LVCMOS33 } [get_ports { led[3] }]; #IO_L23P_T3_35 Sch=led[3]
				
			Synthesized Design Panel
				Bottom Panel > I/O Ports Tab
					Expand "leds" and expand "Package Pin" column and key in the numbers that follows Master XDC file above
						leds[0]: R14
						leds[1]: P14
						leds[2]: N16
						leds[3]: M14
				Save file and "Save Constraints" window will appear
					File Type: XDC
					File name: pin_constraints
					File Location: <Local to Project>
					
				Open Sources tab and navigate to Constraints > pin_constraints.xdc
					Double click to open text editor
					You'll see only Pin numbers are set. IO standards must also be set [IOSTANDARD LVCMOS33]
					
				Open I/O Ports tab
					In "I/O Std" column, "leds(4)" row, set "I/O std" to "LVCMOS33"
					Save project
					Go to Flow Navigator > Constraints > pin_constraints.xdc and click on "Reload"
					
			Navigate to Flow Navigator > Program and Debug
				Click on "Generate Bitstream"
				In Synthesized Design panel, "Synthesized Design is out of date" will appear. 
				Click on "Close Design"
				Bitstream Generation Completed window will pop up. Click on Cancel
				
			Export file
				Open up "Flow Navigator > IP Integrator > Block Design" Panel
					File menu > Export > Export Block Design
					This will be saved in the local project folder as "..jonson_counter/johnson_counter.tcl" file
				
			Connect Board to the PC
				Enter Jupyter Notebook
					Create a folder and name this "johnson_counter"
						Copy Paste design_1_wrapper.bit from ../johnson_counter.runs/impl_1" into this folder and rename as "johnson_counter.bit"
						Copy Paste design_1.tcl from "../johnson_counter" into this folder and rename as "johnson_counter.tcl"
						
					Read up pynq.readthedocs.io/en/v2.1/pynq_libraries/psgpio.html	
						Navigate to Examples section
							from pynq import GPIO
							ps_led = GPIO(GPIO.get_gpio_pin(0), 'out')
							ps_switch = GPIO(GPIO.get_gpio_pin(1), 'out')
							ps_led.write(0)
							ps_switch.read(1)
							
					Open Python3 Terminal and run the code below
						from pynq import Overlay
						ol = Overlay("./johnson_counter.bit")
						
						from pynq import GPIO
						
						left = GPIO(GPIO.get_gpio_pin(0), 'out')
						right = GPIO(GPIO.get_gpio_pin(1), 'out')
						stop = GPIO(GPIO.get_gpio_pin(2), 'out')
						
						left.write(1)
						stop.write(1)
						
						//Editing the clock divider 
						//(Changing the PS clock to the divider instead of changing the clock divider and then rebuilding the design)
						from pynq import Clocks
						Clocks.fclk0_mhz = 10                       //This controls the frequency speed
						print(f'FCLK0: {Clocks.fclk0_mhz:.6f}MHz')  //This will output 10MHz. Change to 1MHz
						
						//Testing for other direction
						right.write(1)
						stop.write(1)
						
					Rename this notebook as johnson_counter.ipynb (File Menu > Rename)
					Save this notebook file
	
		/////////////////////////////////	
		Adding IP to PYNQ Overlay Design (Vivado IP Integrator Design)
			
			Download
				Download Zip file from https://www.github.com/Xilinx/PYNQ
				Extract files and navigate to ../PYNQ-master/boards/Pynq-Z2/base/
					Open base.tcl with Notepad++
					Add the lines below to the bottom of the file [Below create_root_design]
						# Additional steps to get to bitstream
						# Add top wrapper and xdc files
						add_files -norecurse ./vivado/top.v
						update_compile_order -fileset sources_1
						set_property top top [current_fileset]
						update_compile_order -fileset sources_1
						add_files -fileset constrs_1 -norecurse ./vivado/constraints/top.xdc

						# Call implement
						launch_runs impl_1 -to_step write_bitstream -jobs 4
						wait_on_run impl_1

						# This hardware definition file will be used for microblaze projects
						file mkdir ./base/base.sdk
						write_hwdef -force  -file ./base/base.sdk/base.hdf
						file copy -force ./base/base.sdk/base.hdf .

						# Move and rename bitstream to final location
						file copy -force ./base/base.runs/impl_1/top.bit base.bit
						
					Comment out the "launch_runs impl_1 -to_step write_bitstream -jobs 4" code to avoid rebuilding bitstream
					Save this tcl file
					
			Open Vivado
				In Tcl Console, key in:
					cd {C:\PYNQ-master\boards\PYNQ-Z2\base}  //Make sure no spacing in the path or there will be an error
					source ./build_ip.tcl
					source ./base.tcl
				Ignore comment "Run 'impl_1' is not active. Its caused by the line commented out earlier
				
			Editing the Block Diagram
				A diagram window with interconnected blocks will appear 
				Enlarge this window and zoom in to "axi_mem_intercon"
				Remove anything that is not needed. Also make sure to tidy up unconnected ports to "axi_mem_intercon"
					Removing block "trace_analyzer_pmoda"
						Select "axi_mem_intercon" and open Re-customize IP window
							Component Name: axi_mem_intercon
							Number of Slave Interface: 1
							Number of Master Interface: 1
							Interconnect Optimization Strategy: Custom
							Enable Adv Config Options: Unchecked
							Click OK
						Notice that the second S01_AXI is removed from "trace_analyzer_arduino > M_AXI" leaving only one S00_AXI
						Reconnect "trace_analyzer_arduino > M_AXI" to "axi_mem_intercon > S00_AXI"
					Removing everything except for 	block "ps7_0"
						Select all but deselect ps7_0 and the links (DDR, FIXED_IO, and IIC_0)
						Hit Delete
							Confirm Delete Window
								System Nets: Checked
								Sub-blocks: Checked
								Interface Connections: Checked
								Blocks: Checked
								External Interfaces: Checked
								External Ports: Checked
								Click OK
						Tidy up ports
							Remove Slave Ports
								Open Zynq Processing System Window
									Navigate to Page Navigator > PS-PL Configuration
										Navigate to AXI Non Secure Enablement > GP Master AXI Interface 
											M AXI GP0 interface: Checked
											M AXI GP1 Interface: Unchecked
										Navigate to GP Slave AXI Interface 
											S AXI GP0 interface: Unchecked
											S AXI GP1 Interface: Unchecked
										Navigate to HP Slave AXI Interface 
											S AXI HP0 interface: Unchecked
											S AXI HP1 Interface: Unchecked
											S AXI HP2 interface: Unchecked
											S AXI HP3 Interface: Unchecked
										Click OK
							Remove unused clocks		
								Highlight all 3 clocks (FCLK_CLK1, FCLK_CLK2, FCLK_CLK3)
								Open Zynq Processing System Window
									Navigate to Page Navigator > Clock Configuration
										Basic Clocking Tab
											Input Frequency(MHz): 50
											CPU Clock Ration: 6:2:1
											
											PL Fabric Clocks >
												FCLK_CLK0: Checked
												FCLK_CLK1: Unchecked
												FCLK_CLK2: Unchecked
												FCLK_CLK3: Unchecked
									Click OK
							Remove unused Memory
								Open Zynq Processing System Window
									Navigate to Page Navigator > MIO Configuration
										I/O Peripherals > GPIO
											GPIO MIO: Unchecked
											EMIO GPIO(Width): Unchecked
											ENET Reset: Unchecked
											USB Reset: Unchecked
											I2C Reset: Unchecked
											Click OK
							Add Block "+"
								Search for AXI Quad SPI and click on Enter
								Click on "Run Connection Automation"
								Run Connection Automation Window
									Tick the checkbox on "All Automation"
									Click on AXI_LITE
										Master: /ps7_0/M_AXI_GP0
										Interconnect IP: New AXI Interconnect
										Crossbar clock source of Interconnect IP: Auto
										Clock source for Master interface: Auto
										Clock source for Slave interface: Auto
									Click OK
			
		How to Accelerate (Run Program on Programmable Logic Hardware) a Python Function with PYNQ 
			Tutorial: https://www.youtube.com/watch?v=PwG037LuNvA
			IP File: https://www.fpgadeveloper.com/2018/03/how-to-accelerate-a-python-function-with-pynq.html/
			
		Other PYNQ FPGA Projects
			Main PYNQ Repository: https://github.com/Xilinx/PYNQ
			Xilinx PYNQ Networking: https://github.com/Xilinx/PYNQ-Networking
			Xilinx PYNQ Quantized Neural Network: https://github.com/Xilinx/QNN-MO-PYNQ
			Xilinx PYNQ Binary Neural Networks: https://github.com/Xilinx/BNN-PYNQ
			Xilinx PYNQ Computer Vision: https://github.com/Xilinx/PYNQ-ComputerVision
			Xilinx PYNQ Deep Learning: https://github.com/Xilinx/PYNQ-DL
			Xilinx PYNQ Bot: https://github.com/Xilinx/PYNQ-BOT
			Hillhao's PYNQ Neural Networks: https://github.com/hillhao/PYNQ-project
			Awai54st's PYNQ Convolution Neural Networks: https://github.com/awai54st/PYNQ-Classification

	/////////////////////////////////
	5.3 Creating VDMA Overlay with VIVADO and Notebook (AXI Video Direct Memory Access)
		
		Usually used in applications with video pipelines
		Video pipelines includes video source, HDMI input, Filters, controllers, compressions, memories, etc
		https://www.youtube.com/watch?v=XAzu3_bqzqM
		https://xilinx.com/support/documentation/ip_documentation/axi_vdma/v6_2/pg020_axi_vdma.pdf

//////////////////////////////////////////////////////////////////////////////////////////////////
06. Creating Custom Python Function Accelerator on PYNQ with VIVADO tool
	github.com/Xilinx/PYNQ-HelloWorld

	6.1 Accelerate Custom Image Processing Function
		
		In Vivado, create new Project
			Project Name: project_resizer
			Project Location: C:/../resizer_pynq_overlay
			Create project subdirectory: Checkbox ticked
			Click Next
			
			Project Type Window
				Select "RTL Project"
				Do not specify sources at this time: Checkbox ticked
				Click Next
			
			Default Part window
				Select Boards Tab
					Select Pynq-Z2
					Click Next
					
			New Project Summary
				Click Finish


		Settings
			Flow Manager > Project Manager > Settings
				Settings Window	
					Project Settings > IP > Repository
						Click on "+"
						Select the "ip" folder that contains "hls" folder and "resize_1.0" folder
							Refer to the downloaded "resizer_hardware_accelerator" folder
						Click Next
						Add Repository window will pop up
							Expand ">IPs" and you will see "resize_accel"
							Click OK
						Click Apply and the OK

				Bottom Panel
					Open Tcl Console
						Key in: source C:/../../resizer_pynq_overlay/resizer.tcl
						Press Enter
						
				Interconnected block diagram will appear in the IP Integrator> Block Design > Diagram Tab

		HDL Wrapper
			Flow Manager > IP Integrator
				Block Design Window > Sources Tab
					Right click on Design Sources > resizer (resizer.bd)
					Click on "Create HDL Wrapper"

					Create HDL Wrapper Window
						Select "Let Vivado manage wrapper and auto update"
						Click OK
						"Design Sources > resizer (resizer.bd)" will change to "Design Sources > resizer_wrapper (resizer_wrapper.v)"
						
						
		Generate Bitstream (Automatic Synthesis and Implementation)
			Flow Manager > IP Integrator
				Block Design Window > Sources Tab
					Highlight "Design Sources > resizer_wrapper (resizer_wrapper.v)"
					Click on Flow Manager > Program and Debug > Generate Bitstream
						No implementation Results Available window will appear. 
							Click on Yes
						Launch Runs Window will appear
							Launch Directory: <Default Launch Directory>
							Options
								Launch Runs on Local Host: Selected
								Number of jobs: 4
								Generate scripts only: Not selected
							Click OK
						When completed, "Bitstream Generation Completed" window appears
							Click on Cancel
				To watch the current status of Synthesis to see if it has completed
					IP Integrator > Block Design > Bottom Panel > Design Runs Tab
				
		Export Hardware
			Main Menu > File > Export > Export Hardware
				Export Hardware Window
					Include Bitstream: Checkbox ticked
					Export to: <Local to Project>
					Click OK

		Export Bitstream File
			Main Menu > File > Export > Export Bitstream File
				Export Bitstream File Window
					Select "project_resizer" folder
					Save file as resizer_accel.bit

		Export Block Design
			Main Menu > File > Export > Export Block Design
				Export Block Design Window
					Tcl file: C:/../resizer_pynq_overlay/project_resizer/resizer_accel.tcl
					Automatically create top design: Checkbox ticked
					Click OK

		Preparing files to upload to Pynq Z2 Board
			In Windows Explorer, navigate to C:/../resizer_pynq_overlay/project_resizer/
				Duplicate and Rename "resizer_accel.bit" to "resizer.bit"
				Duplicate and Rename "resizer_accel.tcl" to "resizer.tcl"

		Upload files to Pynq Z2 Board
			pynq.readthedocs.io/en/v2.5/pynq_libraries/dma.html  //For send and receive
			Jupyter
				Create a folder and name this "resizer_accelerator"
					Upload tesla.jpg, resizer.bit, resizer.tcl
				Create a notebook Project
					Accelerating the Resizing Function on Software and Hardware
						import numpy as np
						import pandas as pd
						import matplotlib.pyplot as plt
						from PIL import Image
						import cv2
						
						image_path = "./tesla.jpg"
						original_image = cv2.imread(image_path)
						# plt.imshow(original_image)
						
						# showing RGB image using PIL Image function while it can also be showed eith CV2 function
						img_show=Image.open("./tesla.jpg")
						img_show
						
					Software Acceleration Section	
						import time
						print('Software Resize')
						start_time = time.time()
						# Main resizing function running on Software-ARM CPU
						img_lp_sw = cv2.resize(original_image, (150, 75))
						stop_time = time.time()
						sw_exec_time = stop_time - start_time
						print('Software Resize execution time: ', sw_exec_time)
						
						plt.imshow(img_lp_sw)

					Hardware accelerated Resizing
						# Importing the hardware accelerator library
						
						from pynq import Xlnk, Overlay
						
						# loading Bitstream from same directory as this notebook is
						resize_design = Overlay("resizer.bit")
						
						# Printing the Overlay details
						resize_design?  
						
						dma = resize_design.axi_dma_0
						resizer = resize_design.resize_accel_0
						
						# Allocating memory space using the CMA array for our "tesla.jpg" input image. 3 channels, 1133x1700
						# The output buffer will be going to hold the resized image of 75x150
						xlnk = Xlnk()
						in_buffer = xlnk.cma_array(shape=(1133, 1700, 3), dtype=np.uint8, cacheable=1)
						out_buffer = xlnk.cma_array(shape=(75, 150, 3), dtype=np.uint8, cacheable=1)
						
						in_buffer[:] = np.array(original_image)
						
						
						
						#resizer.write(0x10, old_height)
						#resizer.write(0x18, old_width)
						#resizer.write(0x20, new_height)
						#resizer.write(0x28, new_height)
						
						resizer.write(0x10, 1133)
						resizer.write(0x18, 1700)
						resizer.write(0x20, 75)
						resizer.write(0x28, 150)
						
						import time
						print('Hardware Resize')
						start_time = time.time()
						# Main resizing function running on hardware accelerator-PL 
						dma.sendchannel.transfer(in_buffer)
						dma.recvchannel.transfer(out_buffer)
						
						resizer.write(0x00, 0x81) #start
						
						dma.sendchannel.wait()
						dma.recvchannel.wait()
						
						stop_time = time.time()
						hw_exec_time = stop_time - start_time
						print('Hardware Resize execution time: ', hw_exec_time)
						
						img_lp_hw = Image.fromarray(out_buffer)
						plt.imshow(img_lp_hw)
						
						
//////////////////////////////////////////////////////////////////////////////////////////////////
07. Tensorflow Installation on PYNQ FPGA 

	1.Download the PYNQ OS from Pynq.io: 
		Pynq_Z1_Image_2016_09_14 , consists of Python2.7. 
		Update the Python into Python3.4 for installation of tensorflow.

	2.Connect the PYNQ OS to the Router and goto browser and enter: http://pynq:9090

		a. Login and Password both are: xilinx
		b. You do not need to login on Pynq for now, so let’s Log Out on it.

	3.Connect PYNQ Board with USB cable to PC.

		a. Run the Serial Terminal Program: Tera Term or Putty
		b. Goto Serial, Select the COM Port [Tera Term automatically identifies the COM Port but Putty needs to be entered manually; 
			for COM port number, check the Device Manager].
		c. Baud rate is: 115200, and then Enter

	4.Updating:
		$sudo apt-get update

	5.For installation of Python
	
		a.Check the Python installation:
			Enter, $python on your terminal. //This will show the version of python on your Pynq OS. 
			Now do exit(); and enter.
	
		b.For Python 2.7
			$sudo apt-get install python-pip python-dev

		c.For Python 3.3+
			$sudo apt-get install python3-pip python3-dev

		If your python version updated to Python3.4 then you do not need to follow Step d below.
			But this install option will not install the required python [There will be errors for not getting the download location]. 
			So instead, install Python3.4 manually.

		d.Python 3.4 Installation on PYNQ
			a. $wget https://www.python.org/ftp/python/3.4.0/Python-3.4.0.tar.xz
			b. $tar -xf Python-3.4.0.tar.xz
			c. $cd Python-3.4.0
			d. $./configure
			e. $sudo make install

			Installing the Python3.4.0 will take some time, 30 min or more.

			f.Key in:
				$wget https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases/download/v1.1.0/tensorflow-1.1.0-cp34-cp34m-linux_armv7l.whl

			g.Key in:
				$sudo pip3 install tensorflow-1.1.0-cp34-cp34m-linux_armv7l.whl

			h.Installation completes message will appear [maximum 10-20 minutes or earlier].
			i.Go to browser, enter  http://pynq:9090, password is: xilinx
			j.Create New python 3 File from New [right hand side].
			k.Write this down and check the output

//////////////////////////////////////////////////////////////////////////////////////////////////
08. Machine Learning with Xilinx Deep Learning (DPU) IP on PYNQ 

	/////////////////////////////////
	8.1 DPU on PYNQ Boards: Ultra96/ZCU104/ZCU111















//////////////////////////////////////////////////////////////////////////////////////////////////
